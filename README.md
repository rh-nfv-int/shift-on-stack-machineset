# shift-on-stack-machineset

This Ansible project generates a MachineSet Kubernetes object that can be used
to provision OpenShift worker nodes running on top of OpenStack.  

## Prerequisites

1. Ansible 2.9+
2. Provide a valid **OpenStack** *clouds.yaml* file in the searchpath
3. The **openstack.ansible.cloud** module:

   ```bash
      ansible-galaxy collection install openstack.cloud
   ```

4. `metadata.json` file created by the Openshift cluster install

## Configuration

An *inventory.yaml* file provides user-definable parameters related to generating a MachineSet.

### number of_replicas

Number of worker nodes to create

``number_of_replicas: 2``

### cluster_metadata_path

Location of metadata.json file generated by the openshift-installer after 
cluster deployment

``cluster_metadata_path: "metadata.json"``

### openstack_cloud

Specify the cloud to use in the clouds.yaml file.

``openstack_cloud: "openstack"``

### nova_flavor

OpenStack flavor to use for the creation of the nodes 

``nova_flavor: "ocp-worker"``

### node_role

Name of the node role for this machinset

``node_role: "worker"``

### glance_image_name_or_location

Name of glance image to use.  Defaults to `"{{ infraID }}-rhcos"`

``glance_image_name_or_location: "path-to-metadata.json"``

### machines_subnet_UUID

Subnet ID for openshift cluster network. Defaults to `"{{ metadata.infraID }}-openshift"`

``machines_subnet_UUID: "6a295ed7-9464-43d1-8523-bcd6f7711370"``

### additional_networks

The following shows the specification of additional networks in the inventory.yaml file.  The
user can specify either the network UUID or the network name.  In either case, OpenStack is
queried to determine if the specified network is present.  In the case of the network name
being specified, the queried UUID is substituted for the MachineSet file.

The *vnic_type* field can be either **normal** or **direct**.  When connecting an OVS-DPDK
based network, *vnic_type* should be **normal**.  For an SR-IOV connection, specify *vnic_type* as
**direct**.

The *driver* field controls whether or not the interface driver within the VM worker will be 
switched to *vfio-pci* or will remain with the default driver (*virtio*).  If the *driver* field
is not specified, the driver will remain the default.

The *tags* field and associated list allow the user to specify tags that will be added to the 
OpenStack port object.  

Please refer to the [documentation](https://docs.openshift.com/container-platform/4.8/machine_management/creating_machinesets/creating-machineset-osp.html#machineset-yaml-osp-sr-iov_creating-machineset-osp)
for more information.

```yaml
      additional_networks:
      - network_UUID: "60c4b0dd-065a-4f70-8eee-5e7c4a1b8b09"
        name_suffix: "uplink1"
        tags:
          - tag1
        vnic_type: "normal"
      - name: "uplink2"
        name_suffix: "uplink2"
        tags:
          - tag1
        vnic_type: "normal"
        driver: "vfio-pci"
      - network_UUID: "489b4c4d-86ea-4fd2-9a58-b8ec878b5a4c"
        name_suffix: "radio_downlink"
        tags:
          - tag1
        vnic_type: "direct"
        driver: "vfio-pci"
      - name: "radio_uplink"
        name_suffix: "radio_uplink"
        tags:
          - tag1
        vnic_type: "direct"
```

## Running the playbook

An example of running the playbook is below:

``ansible-playbook play.yaml -e cluster_metadata_path=/home/kni/sos-fdp/build-artifacts/metadata.json -i ./inventory.yaml``

The command line references the following metadata.json file:

```json
{
  "clusterName": "fdp",
  "clusterID": "61fa93c1-a958-4ec1-a526-2f0b9272538a",
  "infraID": "fdp-5sd65",
  "openstack": {
    "cloud": "openstack",
    "identifier": {
      "openshiftClusterID": "fdp-5sd65"
    }
  }
}
```

The corresponding MachineSet file will be created in: ``./build/${infraID}-${worker_role}-machineset.yaml``

## Deploying the MachineSet

``oc apply -f ./build/${infraID}-${worker_role}-machineset.yaml``

